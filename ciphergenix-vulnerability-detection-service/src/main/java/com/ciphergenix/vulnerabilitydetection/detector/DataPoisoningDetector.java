package com.ciphergenix.vulnerabilitydetection.detector;

import com.ciphergenix.vulnerabilitydetection.model.DetectionResult;
import com.ciphergenix.vulnerabilitydetection.model.DetectionType;
import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest;
import org.apache.commons.math3.stat.descriptive.DescriptiveStatistics;
import org.apache.commons.math3.linear.RealMatrix;
import org.apache.commons.math3.linear.Array2DRowRealMatrix;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;
import smile.anomaly.IsolationForest;
import smile.clustering.DBSCAN;
import smile.neighbor.LocalOutlierFactor;

import java.util.*;
import java.util.stream.IntStream;

@Component
public class DataPoisoningDetector {
    
    private static final Logger logger = LoggerFactory.getLogger(DataPoisoningDetector.class);
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.contamination-threshold:0.1}")
    private double contaminationThreshold;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.ensemble-weights.isolation-forest:0.3}")
    private double isolationForestWeight;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.ensemble-weights.one-class-svm:0.25}")
    private double oneClassSvmWeight;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.ensemble-weights.autoencoder:0.25}")
    private double autoencoderWeight;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.ensemble-weights.lof:0.2}")
    private double lofWeight;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.statistical-tests.ks-test-threshold:0.05}")
    private double ksTestThreshold;
    
    @Value("${ciphergenix.vulnerability-detection.data-poisoning.statistical-tests.jensen-shannon-threshold:0.1}")
    private double jensenShannonThreshold;
    
    private IsolationForest isolationForest;
    private LocalOutlierFactor lofDetector;
    private KolmogorovSmirnovTest ksTest;
    
    public DataPoisoningDetector() {
        this.ksTest = new KolmogorovSmirnovTest();
        logger.info("DataPoisoningDetector initialized");
    }
    
    /**
     * Main detection method that combines multiple algorithms for robust poisoning detection
     */
    public DetectionResult detectPoisoning(String sessionId, List<List<Double>> dataset, 
                                         Map<String, Object> baselineStats) {
        long startTime = System.currentTimeMillis();
        logger.info("Starting data poisoning detection for session: {}", sessionId);
        
        try {
            // Convert dataset to matrix format
            double[][] dataMatrix = convertToMatrix(dataset);
            
            // Initialize detection result
            DetectionResult result = new DetectionResult(sessionId, DetectionType.DATA_POISONING, 
                                                       0.0, false, "Ensemble");
            result.setDatasetSize(dataset.size());
            
            // Perform statistical anomaly detection
            Map<String, Object> statisticalResults = performStatisticalAnomalyDetection(dataMatrix);
            
            // Perform feature distribution analysis
            Map<String, Object> distributionResults = performFeatureDistributionAnalysis(dataMatrix, baselineStats);
            
            // Perform ensemble anomaly detection
            Map<String, Object> ensembleResults = performEnsembleAnomalyDetection(dataMatrix);
            
            // Combine results and calculate final threat score
            double finalThreatScore = calculateEnsembleThreatScore(statisticalResults, 
                                                                 distributionResults, 
                                                                 ensembleResults);
            
            // Determine if threat is detected
            boolean isThreatDetected = finalThreatScore > contaminationThreshold;
            
            // Collect anomalous samples
            List<Integer> anomalousSamples = identifyAnomalousSamples(dataMatrix, ensembleResults);
            
            // Build detection details
            Map<String, String> detectionDetails = buildDetectionDetails(statisticalResults, 
                                                                        distributionResults, 
                                                                        ensembleResults);
            
            // Update result
            result.setThreatScore(finalThreatScore);
            result.setIsThreatDetected(isThreatDetected);
            result.setAnomalousSamples(anomalousSamples);
            result.setDetectionDetails(detectionDetails);
            result.setConfidenceScore(calculateConfidenceScore(finalThreatScore));
            result.setProcessingTimeMs(System.currentTimeMillis() - startTime);
            
            logger.info("Data poisoning detection completed for session: {}. Threat detected: {}, Score: {}", 
                       sessionId, isThreatDetected, finalThreatScore);
            
            return result;
            
        } catch (Exception e) {
            logger.error("Error during data poisoning detection for session: {}", sessionId, e);
            DetectionResult errorResult = new DetectionResult(sessionId, DetectionType.DATA_POISONING, 
                                                            1.0, true, "Error");
            errorResult.setProcessingTimeMs(System.currentTimeMillis() - startTime);
            Map<String, String> errorDetails = new HashMap<>();
            errorDetails.put("error", e.getMessage());
            errorResult.setDetectionDetails(errorDetails);
            return errorResult;
        }
    }
    
    /**
     * Perform statistical anomaly detection using isolation forests and one-class SVM
     */
    private Map<String, Object> performStatisticalAnomalyDetection(double[][] dataMatrix) {
        Map<String, Object> results = new HashMap<>();
        
        try {
            // Isolation Forest Detection
            isolationForest = IsolationForest.fit(dataMatrix, 100, 0.1);
            int[] outliers = isolationForest.predict(dataMatrix);
            double isolationAnomalyRate = Arrays.stream(outliers).mapToDouble(x -> x == -1 ? 1.0 : 0.0).average().orElse(0.0);
            
            results.put("isolationForestAnomalyRate", isolationAnomalyRate);
            results.put("isolationForestOutliers", outliers);
            
            // Local Outlier Factor (LOF) Detection
            lofDetector = LocalOutlierFactor.of(dataMatrix, 5);
            double[] lofScores = new double[dataMatrix.length];
            for (int i = 0; i < dataMatrix.length; i++) {
                lofScores[i] = lofDetector.lof(dataMatrix[i]);
            }
            
            double avgLofScore = Arrays.stream(lofScores).average().orElse(1.0);
            results.put("avgLofScore", avgLofScore);
            results.put("lofScores", lofScores);
            
            logger.debug("Statistical anomaly detection completed. Isolation anomaly rate: {}, Avg LOF score: {}", 
                        isolationAnomalyRate, avgLofScore);
            
        } catch (Exception e) {
            logger.error("Error in statistical anomaly detection", e);
            results.put("error", e.getMessage());
        }
        
        return results;
    }
    
    /**
     * Perform feature distribution analysis using Kolmogorov-Smirnov tests and Jensen-Shannon divergence
     */
    private Map<String, Object> performFeatureDistributionAnalysis(double[][] dataMatrix, 
                                                                  Map<String, Object> baselineStats) {
        Map<String, Object> results = new HashMap<>();
        
        try {
            if (baselineStats == null || baselineStats.isEmpty()) {
                logger.warn("No baseline statistics provided, computing from current dataset");
                baselineStats = computeBaselineStatistics(dataMatrix);
            }
            
            // Kolmogorov-Smirnov test for each feature
            List<Double> ksTestPValues = new ArrayList<>();
            int numFeatures = dataMatrix[0].length;
            
            for (int feature = 0; feature < numFeatures; feature++) {
                double[] featureData = extractFeatureColumn(dataMatrix, feature);
                
                // Generate baseline distribution (assuming normal distribution from baseline stats)
                double[] baselineData = generateBaselineDistribution(baselineStats, feature, featureData.length);
                
                double pValue = ksTest.kolmogorovSmirnovTest(featureData, baselineData);
                ksTestPValues.add(pValue);
            }
            
            // Calculate distribution shift score
            double avgKsPValue = ksTestPValues.stream().mapToDouble(Double::doubleValue).average().orElse(1.0);
            double distributionShiftScore = 1.0 - avgKsPValue; // Lower p-value indicates higher shift
            
            // Jensen-Shannon divergence calculation
            double jsdivergence = calculateJensenShannonDivergence(dataMatrix, baselineStats);
            
            results.put("ksTestPValues", ksTestPValues);
            results.put("avgKsPValue", avgKsPValue);
            results.put("distributionShiftScore", distributionShiftScore);
            results.put("jensenShannonDivergence", jsdivergence);
            
            logger.debug("Feature distribution analysis completed. Avg KS p-value: {}, JS divergence: {}", 
                        avgKsPValue, jsdivergence);
            
        } catch (Exception e) {
            logger.error("Error in feature distribution analysis", e);
            results.put("error", e.getMessage());
        }
        
        return results;
    }
    
    /**
     * Perform ensemble anomaly detection combining multiple algorithms
     */
    private Map<String, Object> performEnsembleAnomalyDetection(double[][] dataMatrix) {
        Map<String, Object> results = new HashMap<>();
        
        try {
            // DBSCAN clustering for anomaly detection
            DBSCAN<double[]> dbscan = DBSCAN.of(dataMatrix, 10, 0.5);
            int[] clusters = dbscan.y;
            int noisePoints = (int) Arrays.stream(clusters).filter(x -> x == -1).count();
            double dbscanAnomalyRate = (double) noisePoints / dataMatrix.length;
            
            results.put("dbscanAnomalyRate", dbscanAnomalyRate);
            results.put("dbscanClusters", clusters);
            
            // Autoencoder-based anomaly detection (simplified implementation)
            double[] reconstructionErrors = calculateReconstructionErrors(dataMatrix);
            double avgReconstructionError = Arrays.stream(reconstructionErrors).average().orElse(0.0);
            
            results.put("avgReconstructionError", avgReconstructionError);
            results.put("reconstructionErrors", reconstructionErrors);
            
            logger.debug("Ensemble anomaly detection completed. DBSCAN anomaly rate: {}, Avg reconstruction error: {}", 
                        dbscanAnomalyRate, avgReconstructionError);
            
        } catch (Exception e) {
            logger.error("Error in ensemble anomaly detection", e);
            results.put("error", e.getMessage());
        }
        
        return results;
    }
    
    /**
     * Calculate final ensemble threat score using weighted voting
     */
    private double calculateEnsembleThreatScore(Map<String, Object> statisticalResults,
                                              Map<String, Object> distributionResults,
                                              Map<String, Object> ensembleResults) {
        double totalScore = 0.0;
        
        // Isolation Forest score
        Double isolationRate = (Double) statisticalResults.get("isolationForestAnomalyRate");
        if (isolationRate != null) {
            totalScore += isolationForestWeight * isolationRate;
        }
        
        // LOF score (normalized)
        Double avgLofScore = (Double) statisticalResults.get("avgLofScore");
        if (avgLofScore != null) {
            double normalizedLofScore = Math.min(1.0, Math.max(0.0, (avgLofScore - 1.0) / 2.0));
            totalScore += lofWeight * normalizedLofScore;
        }
        
        // Distribution shift score
        Double distributionShiftScore = (Double) distributionResults.get("distributionShiftScore");
        if (distributionShiftScore != null) {
            totalScore += oneClassSvmWeight * distributionShiftScore;
        }
        
        // Autoencoder reconstruction error score (normalized)
        Double avgReconstructionError = (Double) ensembleResults.get("avgReconstructionError");
        if (avgReconstructionError != null) {
            double normalizedReconstructionScore = Math.min(1.0, avgReconstructionError / 0.1);
            totalScore += autoencoderWeight * normalizedReconstructionScore;
        }
        
        return Math.min(1.0, totalScore);
    }
    
    /**
     * Identify anomalous samples from ensemble results
     */
    private List<Integer> identifyAnomalousSamples(double[][] dataMatrix, Map<String, Object> ensembleResults) {
        Set<Integer> anomalousIndices = new HashSet<>();
        
        // From DBSCAN clustering
        int[] clusters = (int[]) ensembleResults.get("dbscanClusters");
        if (clusters != null) {
            for (int i = 0; i < clusters.length; i++) {
                if (clusters[i] == -1) { // Noise points
                    anomalousIndices.add(i);
                }
            }
        }
        
        // From reconstruction errors (top 10% highest errors)
        double[] reconstructionErrors = (double[]) ensembleResults.get("reconstructionErrors");
        if (reconstructionErrors != null) {
            int topCount = Math.max(1, (int) (reconstructionErrors.length * 0.1));
            Integer[] sortedIndices = IntStream.range(0, reconstructionErrors.length)
                    .boxed()
                    .sorted((i, j) -> Double.compare(reconstructionErrors[j], reconstructionErrors[i]))
                    .toArray(Integer[]::new);
            
            for (int i = 0; i < topCount; i++) {
                anomalousIndices.add(sortedIndices[i]);
            }
        }
        
        return new ArrayList<>(anomalousIndices);
    }
    
    /**
     * Build detailed detection information
     */
    private Map<String, String> buildDetectionDetails(Map<String, Object> statisticalResults,
                                                     Map<String, Object> distributionResults,
                                                     Map<String, Object> ensembleResults) {
        Map<String, String> details = new HashMap<>();
        
        details.put("isolationForestAnomalyRate", 
                   String.valueOf(statisticalResults.get("isolationForestAnomalyRate")));
        details.put("avgLofScore", 
                   String.valueOf(statisticalResults.get("avgLofScore")));
        details.put("distributionShiftScore", 
                   String.valueOf(distributionResults.get("distributionShiftScore")));
        details.put("jensenShannonDivergence", 
                   String.valueOf(distributionResults.get("jensenShannonDivergence")));
        details.put("dbscanAnomalyRate", 
                   String.valueOf(ensembleResults.get("dbscanAnomalyRate")));
        details.put("avgReconstructionError", 
                   String.valueOf(ensembleResults.get("avgReconstructionError")));
        
        return details;
    }
    
    /**
     * Calculate confidence score based on threat score
     */
    private double calculateConfidenceScore(double threatScore) {
        // Simple confidence calculation - can be made more sophisticated
        if (threatScore > 0.8) return 0.95;
        if (threatScore > 0.6) return 0.85;
        if (threatScore > 0.4) return 0.75;
        if (threatScore > 0.2) return 0.65;
        return 0.5;
    }
    
    // Helper methods
    
    private double[][] convertToMatrix(List<List<Double>> dataset) {
        int rows = dataset.size();
        int cols = dataset.get(0).size();
        double[][] matrix = new double[rows][cols];
        
        for (int i = 0; i < rows; i++) {
            List<Double> row = dataset.get(i);
            for (int j = 0; j < cols; j++) {
                matrix[i][j] = row.get(j);
            }
        }
        
        return matrix;
    }
    
    private double[] extractFeatureColumn(double[][] matrix, int featureIndex) {
        double[] column = new double[matrix.length];
        for (int i = 0; i < matrix.length; i++) {
            column[i] = matrix[i][featureIndex];
        }
        return column;
    }
    
    private double[] generateBaselineDistribution(Map<String, Object> baselineStats, 
                                                int featureIndex, int size) {
        // Simplified baseline generation - in practice, this would use stored baseline statistics
        Random random = new Random(42);
        double[] baseline = new double[size];
        for (int i = 0; i < size; i++) {
            baseline[i] = random.nextGaussian();
        }
        return baseline;
    }
    
    private Map<String, Object> computeBaselineStatistics(double[][] dataMatrix) {
        Map<String, Object> stats = new HashMap<>();
        
        int numFeatures = dataMatrix[0].length;
        for (int feature = 0; feature < numFeatures; feature++) {
            double[] featureData = extractFeatureColumn(dataMatrix, feature);
            DescriptiveStatistics descriptiveStats = new DescriptiveStatistics(featureData);
            
            Map<String, Double> featureStats = new HashMap<>();
            featureStats.put("mean", descriptiveStats.getMean());
            featureStats.put("std", descriptiveStats.getStandardDeviation());
            featureStats.put("min", descriptiveStats.getMin());
            featureStats.put("max", descriptiveStats.getMax());
            
            stats.put("feature_" + feature, featureStats);
        }
        
        return stats;
    }
    
    private double calculateJensenShannonDivergence(double[][] dataMatrix, Map<String, Object> baselineStats) {
        // Simplified JS divergence calculation
        // In practice, this would compute proper probability distributions and JS divergence
        return 0.1; // Placeholder
    }
    
    private double[] calculateReconstructionErrors(double[][] dataMatrix) {
        // Simplified autoencoder reconstruction error calculation
        // In practice, this would use a trained autoencoder model
        double[] errors = new double[dataMatrix.length];
        Random random = new Random(42);
        
        for (int i = 0; i < dataMatrix.length; i++) {
            errors[i] = random.nextDouble() * 0.1; // Simulated reconstruction error
        }
        
        return errors;
    }
}